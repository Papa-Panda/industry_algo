{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/industry_algo/blob/main/Deep_Interest_Evolution_Network_(DIEN)_Implementation_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Synthetic Data Generation ---\n",
        "# Reusing the data generation from the DIN PyTorch example.\n",
        "\n",
        "def generate_synthetic_data(num_samples=10000, num_users=1000, num_items=500, history_length=10, embedding_dim=16):\n",
        "    \"\"\"\n",
        "    Generates synthetic data for a Deep Interest Evolution Network.\n",
        "    \"\"\"\n",
        "    user_ids = np.random.randint(1, num_users + 1, num_samples)\n",
        "    candidate_item_ids = np.random.randint(1, num_items + 1, num_samples)\n",
        "\n",
        "    historical_item_ids = []\n",
        "    for _ in range(num_samples):\n",
        "        current_history_len = np.random.randint(1, history_length + 1)\n",
        "        history = np.random.randint(1, num_items + 1, current_history_len).tolist()\n",
        "        history.extend([0] * (history_length - len(history))) # Pad with 0s\n",
        "        historical_item_ids.append(history)\n",
        "    historical_item_ids = np.array(historical_item_ids)\n",
        "\n",
        "    labels = np.random.randint(0, 2, num_samples)\n",
        "\n",
        "    item_features_matrix = np.random.rand(num_items + 1, embedding_dim).astype(np.float32)\n",
        "\n",
        "    print(f\"Generated synthetic data: {num_samples} samples.\")\n",
        "    print(f\"  User IDs shape: {user_ids.shape}\")\n",
        "    print(f\"  Candidate Item IDs shape: {candidate_item_ids.shape}\")\n",
        "    print(f\"  Historical Item IDs shape: {historical_item_ids.shape}\")\n",
        "    print(f\"  Labels shape: {labels.shape}\")\n",
        "    print(f\"  Item Features matrix shape: {item_features_matrix.shape}\")\n",
        "\n",
        "    user_ids_t = torch.LongTensor(user_ids)\n",
        "    candidate_item_ids_t = torch.LongTensor(candidate_item_ids)\n",
        "    historical_item_ids_t = torch.LongTensor(historical_item_ids)\n",
        "    labels_t = torch.FloatTensor(labels).unsqueeze(1)\n",
        "\n",
        "    return user_ids_t, candidate_item_ids_t, historical_item_ids_t, labels_t, item_features_matrix\n",
        "\n",
        "# Generate the data\n",
        "num_users = 1000\n",
        "num_items = 500\n",
        "history_length = 10\n",
        "embedding_dim = 16\n",
        "gru_units = 32 # Units for GRU layers\n",
        "\n",
        "user_ids_data, candidate_item_ids_data, historical_item_ids_data, labels_data, item_features_data_np = \\\n",
        "    generate_synthetic_data(num_samples=50000, num_users=num_users, num_items=num_items, history_length=history_length, embedding_dim=embedding_dim)\n",
        "\n",
        "# --- 2. DIEN Model Architecture in PyTorch ---\n",
        "\n",
        "# Custom Activation Function (Dice) - Reused from DIN example\n",
        "class Dice(nn.Module):\n",
        "    \"\"\"\n",
        "    Data Adaptive Activation Function (DICE) for DIEN in PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, epsilon=1e-9):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.alphas = nn.Parameter(torch.zeros(input_dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(input_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        reduction_axes = tuple(range(x.dim() - 1))\n",
        "        mean = torch.mean(x, dim=reduction_axes, keepdim=True)\n",
        "        variance = torch.mean(torch.pow(x - mean, 2), dim=reduction_axes, keepdim=True)\n",
        "        x_normed = (x - mean) / torch.sqrt(variance + self.epsilon)\n",
        "        p = torch.sigmoid(self.alphas * x_normed + self.beta)\n",
        "        return p * x + (1 - p) * self.alphas * x\n",
        "\n",
        "# Attention Layer for DIEN's Interest Evolving Layer\n",
        "class DIENAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism for DIEN's interest evolving layer.\n",
        "    Computes attention weights for each historical state w.r.t. the candidate item.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, attention_hidden_units=[32, 16]):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.attention_hidden_units = attention_hidden_units\n",
        "\n",
        "        attention_mlp_layers = []\n",
        "        # Input dim: candidate_emb + history_state + product + difference\n",
        "        input_dim_mlp = embedding_dim * 4\n",
        "\n",
        "        for i, units in enumerate(attention_hidden_units):\n",
        "            attention_mlp_layers.append(nn.Linear(input_dim_mlp, units))\n",
        "            attention_mlp_layers.append(Dice(units))\n",
        "            input_dim_mlp = units\n",
        "\n",
        "        self.attention_mlp = nn.Sequential(*attention_mlp_layers)\n",
        "        self.output_layer = nn.Linear(input_dim_mlp, 1) # Outputs a single attention score\n",
        "\n",
        "    def forward(self, candidate_item_embedding, historical_sequence_states):\n",
        "        # candidate_item_embedding: (batch_size, embedding_dim)\n",
        "        # historical_sequence_states: (batch_size, seq_len, gru_units) from the first GRU\n",
        "\n",
        "        batch_size, seq_len, _ = historical_sequence_states.shape\n",
        "\n",
        "        # Expand candidate_item_embedding to match seq_len for concatenation\n",
        "        candidate_item_embedding_tiled = candidate_item_embedding.unsqueeze(1).expand(-1, seq_len, -1)\n",
        "\n",
        "        # Concatenate features for attention MLP\n",
        "        concatenated_features = torch.cat([\n",
        "            candidate_item_embedding_tiled,\n",
        "            historical_sequence_states,\n",
        "            candidate_item_embedding_tiled * historical_sequence_states,\n",
        "            candidate_item_embedding_tiled - historical_sequence_states\n",
        "        ], dim=-1)\n",
        "\n",
        "        # Apply attention MLP\n",
        "        attention_logits = self.attention_mlp(concatenated_features) # (batch_size, seq_len, 1)\n",
        "        attention_logits = self.output_layer(attention_logits) # (batch_size, seq_len, 1)\n",
        "\n",
        "        # Mask out padded items (where historical_sequence_states are all zeros)\n",
        "        mask = (historical_sequence_states.abs().sum(dim=-1, keepdim=True) > 0).float()\n",
        "        attention_logits = attention_logits - (1.0 - mask) * 1e9 # Apply large negative value to masked logits\n",
        "\n",
        "        attention_weights = F.softmax(attention_logits, dim=1) # (batch_size, seq_len, 1)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "# Attention-Guided GRU Layer (Simplified AUGRU concept)\n",
        "class AttentionGRULayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified implementation of the Attention Update Gate (AUGRU) concept for DIEN.\n",
        "    It applies attention weights to the input sequence before feeding it into a standard GRU.\n",
        "    \"\"\"\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        # Standard GRU layer\n",
        "        self.gru = nn.GRU(input_size=units, hidden_size=units, batch_first=True)\n",
        "\n",
        "    def forward(self, input_sequence_embedding, attention_weights, initial_state=None):\n",
        "        # input_sequence_embedding: (batch_size, seq_len, units) from the first GRU's output\n",
        "        # attention_weights: (batch_size, seq_len, 1) from DIENAttention layer\n",
        "\n",
        "        # Apply attention weights to the input sequence\n",
        "        # This effectively modulates the input to the GRU based on attention.\n",
        "        attention_weighted_inputs = input_sequence_embedding * attention_weights\n",
        "\n",
        "        # Process the attention-weighted sequence with the GRU\n",
        "        # If initial_state is None, GRU initializes it with zeros.\n",
        "        _, final_state = self.gru(attention_weighted_inputs, initial_state)\n",
        "\n",
        "        # final_state is (1, batch_size, units) for non-bidirectional GRU\n",
        "        return final_state.squeeze(0) # (batch_size, units)\n",
        "\n",
        "# Deep Interest Evolution Network (DIEN) Model\n",
        "class DIEN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, history_length, embedding_dim, gru_units, item_features_matrix):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.history_length = history_length\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.gru_units = gru_units\n",
        "\n",
        "        # Embedding layers\n",
        "        self.user_embedding = nn.Embedding(num_users + 1, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding.from_pretrained(\n",
        "            torch.from_numpy(item_features_matrix).float(),\n",
        "            freeze=False\n",
        "        )\n",
        "\n",
        "        # Interest Extractor Layer: GRU to get sequential interest states\n",
        "        # input_size=embedding_dim (from item embeddings), hidden_size=gru_units\n",
        "        # In PyTorch, return_sequences=True is implied when batch_first=True and you take the first output of GRU\n",
        "        self.interest_extractor_gru = nn.GRU(input_size=embedding_dim, hidden_size=gru_units, batch_first=True)\n",
        "\n",
        "        # Interest Evolving Layer components\n",
        "        self.dien_attention = DIENAttention(embedding_dim=gru_units, attention_hidden_units=[32, 16]) # Attention over GRU states\n",
        "        self.interest_evolving_gru = AttentionGRULayer(units=gru_units) # Attention-guided GRU\n",
        "\n",
        "        # Final Prediction MLP\n",
        "        # Input dim: user_emb + candidate_item_emb + evolved_interest_state\n",
        "        input_mlp_dim = embedding_dim + embedding_dim + gru_units\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_mlp_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.output_layer = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, user_id, candidate_item_id, historical_item_ids):\n",
        "        user_id = user_id.squeeze(-1)\n",
        "        candidate_item_id = candidate_item_id.squeeze(-1)\n",
        "\n",
        "        user_emb = self.user_embedding(user_id)\n",
        "        candidate_item_emb = self.item_embedding(candidate_item_id)\n",
        "        historical_item_embs = self.item_embedding(historical_item_ids)\n",
        "\n",
        "        # Interest Extractor Layer\n",
        "        # historical_item_embs: (batch_size, history_length, embedding_dim)\n",
        "        # interest_states: (batch_size, history_length, gru_units)\n",
        "        interest_states, _ = self.interest_extractor_gru(historical_item_embs)\n",
        "\n",
        "        # Interest Evolving Layer\n",
        "        # Project candidate_item_emb to gru_units before passing to attention\n",
        "        candidate_item_emb_proj = nn.Linear(self.embedding_dim, self.gru_units).to(device)(candidate_item_emb)\n",
        "\n",
        "        attention_weights = self.dien_attention(candidate_item_emb_proj, interest_states) # (batch_size, history_length, 1)\n",
        "\n",
        "        # 2. Evolve Interest using AttentionGRULayer\n",
        "        evolved_interest_state = self.interest_evolving_gru(interest_states, attention_weights) # (batch_size, gru_units)\n",
        "\n",
        "        # Concatenate all features\n",
        "        concatenated_features = torch.cat([\n",
        "            user_emb,\n",
        "            candidate_item_emb,\n",
        "            evolved_interest_state\n",
        "        ], dim=-1)\n",
        "\n",
        "        # Pass through prediction MLP\n",
        "        mlp_output = self.mlp(concatenated_features)\n",
        "        logits = self.output_layer(mlp_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Instantiate and move model to device\n",
        "dien_model = DIEN(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    history_length=history_length,\n",
        "    embedding_dim=embedding_dim,\n",
        "    gru_units=gru_units,\n",
        "    item_features_matrix=item_features_data_np\n",
        ").to(device)\n",
        "\n",
        "print(\"\\nDIEN Model Summary:\")\n",
        "print(dien_model)\n",
        "\n",
        "\n",
        "# --- 3. Prepare Data for Training (PyTorch DataLoader) ---\n",
        "dataset = TensorDataset(user_ids_data, candidate_item_ids_data, historical_item_ids_data, labels_data)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- 4. Train the Model ---\n",
        "optimizer = torch.optim.Adam(dien_model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "print(f\"\\n--- Training the DIEN Model for {num_epochs} epochs ---\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    dien_model.train()\n",
        "    total_loss = 0\n",
        "    predictions_train = []\n",
        "    labels_train = []\n",
        "\n",
        "    for batch_idx, (user_ids, candidate_ids, historical_ids, labels) in enumerate(train_loader):\n",
        "        user_ids, candidate_ids, historical_ids, labels = \\\n",
        "            user_ids.to(device), candidate_ids.to(device), historical_ids.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = dien_model(user_ids, candidate_ids, historical_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        predictions_train.extend(outputs.sigmoid().detach().cpu().numpy().flatten())\n",
        "        labels_train.extend(labels.detach().cpu().numpy().flatten())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_accuracy = accuracy_score(labels_train, np.round(predictions_train))\n",
        "    train_auc = roc_auc_score(labels_train, predictions_train)\n",
        "\n",
        "    # --- Validation ---\n",
        "    dien_model.eval()\n",
        "    val_total_loss = 0\n",
        "    predictions_val = []\n",
        "    labels_val = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user_ids, candidate_ids, historical_ids, labels in val_loader:\n",
        "            user_ids, candidate_ids, historical_ids, labels = \\\n",
        "                user_ids.to(device), candidate_ids.to(device), historical_ids.to(device), labels.to(device)\n",
        "\n",
        "            outputs = dien_model(user_ids, candidate_ids, historical_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_total_loss += loss.item()\n",
        "\n",
        "            predictions_val.extend(outputs.sigmoid().cpu().numpy().flatten())\n",
        "            labels_val.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "    avg_val_loss = val_total_loss / len(val_loader)\n",
        "    val_accuracy = accuracy_score(labels_val, np.round(predictions_val))\n",
        "    val_auc = roc_auc_score(labels_val, predictions_val)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train AUC: {train_auc:.4f}\")\n",
        "    print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "\n",
        "# --- 5. Make Predictions (Example) ---\n",
        "print(\"\\n--- Making Predictions (Example) ---\")\n",
        "\n",
        "dien_model.eval()\n",
        "\n",
        "num_predict_samples = 5\n",
        "random_indices = np.random.choice(len(user_ids_data), num_predict_samples, replace=False)\n",
        "\n",
        "sample_user_ids = user_ids_data[random_indices].to(device)\n",
        "sample_candidate_item_ids = candidate_item_ids_data[random_indices].to(device)\n",
        "sample_historical_item_ids = historical_item_ids_data[random_indices].to(device)\n",
        "sample_labels = labels_data[random_indices].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_outputs = dien_model(sample_user_ids, sample_candidate_item_ids, sample_historical_item_ids)\n",
        "    sample_predictions = sample_outputs.sigmoid().cpu().numpy().flatten()\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(num_predict_samples):\n",
        "    print(f\"  Sample {i+1}:\")\n",
        "    print(f\"    User ID: {sample_user_ids[i].item()}\")\n",
        "    print(f\"    Candidate Item ID: {sample_candidate_item_ids[i].item()}\")\n",
        "    print(f\"    Historical Item IDs: {sample_historical_item_ids[i].cpu().numpy().tolist()}\")\n",
        "    print(f\"    True Label: {sample_labels[i].item()}\")\n",
        "    print(f\"    Predicted Probability: {sample_predictions[i]:.4f}\")\n",
        "    print(\"-\" * 30)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124\n",
            "Using device: cpu\n",
            "Generated synthetic data: 50000 samples.\n",
            "  User IDs shape: (50000,)\n",
            "  Candidate Item IDs shape: (50000,)\n",
            "  Historical Item IDs shape: (50000, 10)\n",
            "  Labels shape: (50000,)\n",
            "  Item Features matrix shape: (501, 16)\n",
            "\n",
            "DIEN Model Summary:\n",
            "DIEN(\n",
            "  (user_embedding): Embedding(1001, 16)\n",
            "  (item_embedding): Embedding(501, 16)\n",
            "  (interest_extractor_gru): GRU(16, 32, batch_first=True)\n",
            "  (dien_attention): DIENAttention(\n",
            "    (attention_mlp): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (1): Dice()\n",
            "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "      (3): Dice()\n",
            "    )\n",
            "    (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
            "  )\n",
            "  (interest_evolving_gru): AttentionGRULayer(\n",
            "    (gru): GRU(32, 32, batch_first=True)\n",
            "  )\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (7): ReLU()\n",
            "  )\n",
            "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "--- Training the DIEN Model for 5 epochs ---\n",
            "Epoch 1/5:\n",
            "  Train Loss: 0.6935, Train Acc: 0.5010, Train AUC: 0.5028\n",
            "  Val Loss: 0.6933, Val Acc: 0.4949, Val AUC: 0.4919\n",
            "Epoch 2/5:\n",
            "  Train Loss: 0.6929, Train Acc: 0.5103, Train AUC: 0.5136\n",
            "  Val Loss: 0.6932, Val Acc: 0.4992, Val AUC: 0.4928\n",
            "Epoch 3/5:\n",
            "  Train Loss: 0.6928, Train Acc: 0.5136, Train AUC: 0.5165\n",
            "  Val Loss: 0.6934, Val Acc: 0.4928, Val AUC: 0.4882\n",
            "Epoch 4/5:\n",
            "  Train Loss: 0.6921, Train Acc: 0.5162, Train AUC: 0.5260\n",
            "  Val Loss: 0.6947, Val Acc: 0.4930, Val AUC: 0.4884\n",
            "Epoch 5/5:\n",
            "  Train Loss: 0.6913, Train Acc: 0.5248, Train AUC: 0.5341\n",
            "  Val Loss: 0.6954, Val Acc: 0.4930, Val AUC: 0.4898\n",
            "\n",
            "Training complete.\n",
            "\n",
            "--- Making Predictions (Example) ---\n",
            "\n",
            "Sample Predictions:\n",
            "  Sample 1:\n",
            "    User ID: 500\n",
            "    Candidate Item ID: 398\n",
            "    Historical Item IDs: [292, 462, 475, 133, 284, 150, 398, 291, 111, 0]\n",
            "    True Label: 0.0\n",
            "    Predicted Probability: 0.4431\n",
            "------------------------------\n",
            "  Sample 2:\n",
            "    User ID: 214\n",
            "    Candidate Item ID: 407\n",
            "    Historical Item IDs: [385, 204, 392, 367, 46, 93, 428, 310, 91, 0]\n",
            "    True Label: 1.0\n",
            "    Predicted Probability: 0.5163\n",
            "------------------------------\n",
            "  Sample 3:\n",
            "    User ID: 608\n",
            "    Candidate Item ID: 279\n",
            "    Historical Item IDs: [440, 371, 479, 396, 396, 456, 56, 342, 34, 0]\n",
            "    True Label: 0.0\n",
            "    Predicted Probability: 0.4866\n",
            "------------------------------\n",
            "  Sample 4:\n",
            "    User ID: 67\n",
            "    Candidate Item ID: 303\n",
            "    Historical Item IDs: [402, 53, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    True Label: 1.0\n",
            "    Predicted Probability: 0.5085\n",
            "------------------------------\n",
            "  Sample 5:\n",
            "    User ID: 488\n",
            "    Candidate Item ID: 139\n",
            "    Historical Item IDs: [440, 66, 330, 295, 317, 0, 0, 0, 0, 0]\n",
            "    True Label: 1.0\n",
            "    Predicted Probability: 0.5056\n",
            "------------------------------\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBzLnTSIu03I",
        "outputId": "9aa71bbd-a3ce-4d41-d7c0-3a8ef6608611"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-PRjZ8zsvKJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}