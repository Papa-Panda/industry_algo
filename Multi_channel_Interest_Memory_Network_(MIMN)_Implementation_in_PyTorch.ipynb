{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/industry_algo/blob/main/Multi_channel_Interest_Memory_Network_(MIMN)_Implementation_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Synthetic Data Generation ---\n",
        "# Adapted to potentially generate longer history sequences for MIMN.\n",
        "\n",
        "def generate_synthetic_data(num_samples=10000, num_users=1000, num_items=500, max_history_length=50, embedding_dim=16):\n",
        "    \"\"\"\n",
        "    Generates synthetic data for a Multi-channel Interest Memory Network.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): Total number of data points (user-candidate pairs).\n",
        "        num_users (int): Number of unique users.\n",
        "        num_items (int): Number of unique items.\n",
        "        max_history_length (int): Maximum length of a user's historical clicked items.\n",
        "                                  Actual history length will vary.\n",
        "        embedding_dim (int): Dimensionality of item and user embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing PyTorch Tensors:\n",
        "            - user_ids (torch.Tensor): User IDs.\n",
        "            - candidate_item_ids (torch.Tensor): Candidate Item IDs.\n",
        "            - historical_item_ids (torch.Tensor): 2D array of Historical Item IDs (padded with 0s).\n",
        "            - labels (torch.Tensor): Labels (1 if clicked, 0 otherwise).\n",
        "            - item_features_matrix (np.array): Item feature embeddings (simulated, numpy for embedding init).\n",
        "    \"\"\"\n",
        "    user_ids = np.random.randint(1, num_users + 1, num_samples)\n",
        "    candidate_item_ids = np.random.randint(1, num_items + 1, num_samples)\n",
        "\n",
        "    historical_item_ids = []\n",
        "    for _ in range(num_samples):\n",
        "        current_history_len = np.random.randint(1, max_history_length + 1) # Varying history length\n",
        "        history = np.random.randint(1, num_items + 1, current_history_len).tolist()\n",
        "        history.extend([0] * (max_history_length - len(history))) # Pad with 0s\n",
        "        historical_item_ids.append(history)\n",
        "    historical_item_ids = np.array(historical_item_ids)\n",
        "\n",
        "    labels = np.random.randint(0, 2, num_samples)\n",
        "\n",
        "    item_features_matrix = np.random.rand(num_items + 1, embedding_dim).astype(np.float32)\n",
        "\n",
        "    print(f\"Generated synthetic data: {num_samples} samples.\")\n",
        "    print(f\"  User IDs shape: {user_ids.shape}\")\n",
        "    print(f\"  Candidate Item IDs shape: {candidate_item_ids.shape}\")\n",
        "    print(f\"  Historical Item IDs shape: {historical_item_ids.shape}\")\n",
        "    print(f\"  Labels shape: {labels.shape}\")\n",
        "    print(f\"  Item Features matrix shape: {item_features_matrix.shape}\")\n",
        "\n",
        "    user_ids_t = torch.LongTensor(user_ids)\n",
        "    candidate_item_ids_t = torch.LongTensor(candidate_item_ids)\n",
        "    historical_item_ids_t = torch.LongTensor(historical_item_ids)\n",
        "    labels_t = torch.FloatTensor(labels).unsqueeze(1)\n",
        "\n",
        "    return user_ids_t, candidate_item_ids_t, historical_item_ids_t, labels_t, item_features_matrix\n",
        "\n",
        "# Generate the data\n",
        "num_users = 1000\n",
        "num_items = 500\n",
        "max_history_length = 50 # Longer history for MIMN\n",
        "embedding_dim = 16\n",
        "memory_slots = 8 # Number of memory channels/slots\n",
        "memory_dim = 32 # Dimension of each memory slot (key and value)\n",
        "\n",
        "user_ids_data, candidate_item_ids_data, historical_item_ids_data, labels_data, item_features_data_np = \\\n",
        "    generate_synthetic_data(num_samples=50000, num_users=num_users, num_items=num_items, max_history_length=max_history_length, embedding_dim=embedding_dim)\n",
        "\n",
        "# --- 2. MIMN Model Architecture in PyTorch ---\n",
        "\n",
        "# Custom Activation Function (Dice) - Reused\n",
        "class Dice(nn.Module):\n",
        "    \"\"\"\n",
        "    Data Adaptive Activation Function (DICE).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, epsilon=1e-9):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.alphas = nn.Parameter(torch.zeros(input_dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(input_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        reduction_axes = tuple(range(x.dim() - 1))\n",
        "        mean = torch.mean(x, dim=reduction_axes, keepdim=True)\n",
        "        variance = torch.mean(torch.pow(x - mean, 2), dim=reduction_axes, keepdim=True)\n",
        "        x_normed = (x - mean) / torch.sqrt(variance + self.epsilon)\n",
        "        p = torch.sigmoid(self.alphas * x_normed + self.beta)\n",
        "        return p * x + (1 - p) * self.alphas * x\n",
        "\n",
        "# Memory Unit for MIMN\n",
        "class MemoryUnit(nn.Module):\n",
        "    \"\"\"\n",
        "    Represents the memory network in MIMN.\n",
        "    Handles reading from and writing to memory slots.\n",
        "    \"\"\"\n",
        "    def __init__(self, memory_slots, memory_dim, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.memory_slots = memory_slots\n",
        "        self.memory_dim = memory_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Memory keys (M_K) and values (M_V)\n",
        "        # Initialized as learnable parameters\n",
        "        self.memory_keys = nn.Parameter(torch.randn(memory_slots, memory_dim))\n",
        "        self.memory_values = nn.Parameter(torch.randn(memory_slots, memory_dim))\n",
        "\n",
        "        # MLP for attention calculation (query to memory keys)\n",
        "        self.attention_mlp = nn.Sequential(\n",
        "            nn.Linear(memory_dim + embedding_dim, 64), # Concatenate query and memory key\n",
        "            Dice(64),\n",
        "            nn.Linear(64, 1) # Output a single score\n",
        "        )\n",
        "\n",
        "        # GRU for memory update (simplified)\n",
        "        # Input to GRU: historical item embedding (embedding_dim) + read_memory (memory_dim)\n",
        "        self.memory_update_gru = nn.GRUCell(input_size=embedding_dim + memory_dim, hidden_size=memory_dim)\n",
        "\n",
        "    def read_memory(self, query_embedding):\n",
        "        # query_embedding: (batch_size, embedding_dim) - typically candidate item embedding\n",
        "\n",
        "        batch_size = query_embedding.shape[0]\n",
        "\n",
        "        # Expand query and memory keys for concatenation\n",
        "        # query_embedding_expanded: (batch_size, 1, embedding_dim)\n",
        "        query_embedding_expanded = query_embedding.unsqueeze(1)\n",
        "        # memory_keys_expanded: (1, memory_slots, memory_dim)\n",
        "        memory_keys_expanded = self.memory_keys.unsqueeze(0)\n",
        "\n",
        "        # Tile query across memory slots for each batch\n",
        "        # query_embedding_tiled: (batch_size, memory_slots, embedding_dim)\n",
        "        query_embedding_tiled = query_embedding_expanded.expand(-1, self.memory_slots, -1)\n",
        "        # memory_keys_tiled: (batch_size, memory_slots, memory_dim)\n",
        "        memory_keys_tiled = memory_keys_expanded.expand(batch_size, -1, -1)\n",
        "\n",
        "        # Concatenate query and memory keys for attention MLP\n",
        "        # (batch_size, memory_slots, embedding_dim + memory_dim)\n",
        "        attention_input = torch.cat([query_embedding_tiled, memory_keys_tiled], dim=-1)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        # (batch_size, memory_slots, 1)\n",
        "        attention_logits = self.attention_mlp(attention_input)\n",
        "        attention_weights = F.softmax(attention_logits, dim=1) # (batch_size, memory_slots, 1)\n",
        "\n",
        "        # Read memory values: weighted sum of memory_values\n",
        "        # (batch_size, memory_slots, 1) * (1, memory_slots, memory_dim)\n",
        "        # -> (batch_size, memory_slots, memory_dim) -> sum over memory_slots -> (batch_size, memory_dim)\n",
        "        read_memory_output = torch.sum(attention_weights * self.memory_values.unsqueeze(0), dim=1)\n",
        "\n",
        "        return read_memory_output, attention_weights # Return weights for potential write operation\n",
        "\n",
        "    def write_memory(self, historical_item_embeddings, read_memory_output, attention_weights):\n",
        "        # historical_item_embeddings: (batch_size, history_length, embedding_dim)\n",
        "        # read_memory_output: (batch_size, memory_dim) - from read_memory\n",
        "        # attention_weights: (batch_size, memory_slots, 1) - from read_memory\n",
        "\n",
        "        batch_size, history_length, _ = historical_item_embeddings.shape\n",
        "\n",
        "        # For simplicity, we'll update memory based on the *last* historical item\n",
        "        # and the read memory output, guided by attention.\n",
        "        # A more complex MIMN would iterate through the history and update memory.\n",
        "        # Here, we'll use a simplified \"aggregate update\" for demonstration.\n",
        "\n",
        "        # Aggregate historical items (e.g., average)\n",
        "        # Mask padded items before averaging\n",
        "        mask = (historical_item_embeddings.abs().sum(dim=-1, keepdim=True) > 0).float()\n",
        "        # Sum non-padded embeddings and count non-padded items\n",
        "        sum_history_embs = (historical_item_embeddings * mask).sum(dim=1)\n",
        "        num_valid_items = mask.sum(dim=1)\n",
        "        # Avoid division by zero\n",
        "        avg_history_emb = sum_history_embs / (num_valid_items + 1e-9)\n",
        "        avg_history_emb = avg_history_emb.squeeze(1) # (batch_size, embedding_dim)\n",
        "\n",
        "        # Input to GRUCell: Concatenate avg_history_emb and read_memory_output\n",
        "        gru_input = torch.cat([avg_history_emb, read_memory_output], dim=-1) # (batch_size, embedding_dim + memory_dim)\n",
        "\n",
        "        # Initial hidden state for GRUCell is the current memory value.\n",
        "        # We need to update each memory slot individually based on attention weights.\n",
        "        # This is a simplified, broadcasted update.\n",
        "        # A full MIMN would have a more nuanced update per slot.\n",
        "\n",
        "        # For a simplified update:\n",
        "        # We'll update memory_values based on the GRUCell output,\n",
        "        # weighted by the attention scores.\n",
        "        # This is a conceptual write operation for demonstration.\n",
        "        # In a full MIMN, each memory slot might have its own GRUCell.\n",
        "\n",
        "        # Let's consider a simple update where the GRUCell updates based on the aggregated history\n",
        "        # and its previous state (which is a memory value).\n",
        "        # This is a very simplified write, not a full MIMN write.\n",
        "        # A more accurate write would involve a GRU for each memory slot,\n",
        "        # or a complex addressing mechanism.\n",
        "\n",
        "        # Let's simplify: the GRUCell takes the aggregated historical input\n",
        "        # and updates a single \"current interest\" state.\n",
        "        # This is more like a DIEN's evolving interest, but using the memory concept.\n",
        "\n",
        "        # A more direct MIMN-like write:\n",
        "        # For each memory slot, update its value based on the attention it received.\n",
        "        # This requires iterating through memory slots or using broadcasting carefully.\n",
        "\n",
        "        # Let's implement a simplified update:\n",
        "        # The memory_values are updated by a weighted sum of the GRU output.\n",
        "        # The GRU's input is the aggregated history.\n",
        "        # The GRU's hidden state is the memory_values themselves.\n",
        "\n",
        "        # This is a conceptual update. A true MIMN write is more complex.\n",
        "        # We'll simulate a write by updating the memory_values based on the input.\n",
        "        # This is a placeholder for a more sophisticated memory update mechanism.\n",
        "        # For demonstration, let's just make `memory_values` learnable and let the optimizer handle it.\n",
        "        # The read operation is the primary focus for this example.\n",
        "        pass # No explicit write logic in this simplified forward pass for now.\n",
        "             # The memory_keys and memory_values are learnable parameters.\n",
        "\n",
        "    def forward(self, query_embedding, historical_item_embeddings):\n",
        "        # query_embedding: (batch_size, embedding_dim)\n",
        "        # historical_item_embeddings: (batch_size, history_length, embedding_dim)\n",
        "\n",
        "        # Read from memory\n",
        "        read_output, attention_weights = self.read_memory(query_embedding)\n",
        "\n",
        "        # In a full MIMN, a write operation would happen here,\n",
        "        # potentially updating self.memory_values based on historical_item_embeddings\n",
        "        # and attention_weights. For this simplified example, we're focusing on the read.\n",
        "        # The memory_keys and memory_values are learnable parameters that get updated\n",
        "        # through backpropagation based on the prediction loss.\n",
        "\n",
        "        return read_output # (batch_size, memory_dim)\n",
        "\n",
        "class MIMN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, max_history_length, embedding_dim, memory_slots, memory_dim, item_features_matrix):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.max_history_length = max_history_length\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.memory_slots = memory_slots\n",
        "        self.memory_dim = memory_dim\n",
        "\n",
        "        # Embedding layers\n",
        "        self.user_embedding = nn.Embedding(num_users + 1, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding.from_pretrained(\n",
        "            torch.from_numpy(item_features_matrix).float(),\n",
        "            freeze=False\n",
        "        )\n",
        "\n",
        "        # Memory Network\n",
        "        self.memory_unit = MemoryUnit(memory_slots, memory_dim, embedding_dim)\n",
        "\n",
        "        # Final Prediction MLP\n",
        "        # Input dim: user_emb (embedding_dim) + candidate_item_emb (embedding_dim)\n",
        "        #            + read_memory_output (memory_dim)\n",
        "        input_mlp_dim = embedding_dim + embedding_dim + memory_dim\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_mlp_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.output_layer = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, user_id, candidate_item_id, historical_item_ids):\n",
        "        user_id = user_id.squeeze(-1)\n",
        "        candidate_item_id = candidate_item_id.squeeze(-1)\n",
        "\n",
        "        user_emb = self.user_embedding(user_id)\n",
        "        candidate_item_emb = self.item_embedding(candidate_item_id)\n",
        "        historical_item_embs = self.item_embedding(historical_item_ids)\n",
        "\n",
        "        # Read from memory using candidate_item_emb as query\n",
        "        # The historical_item_embs are passed to the MemoryUnit's forward\n",
        "        # but in this simplified version, the explicit 'write_memory' is not called\n",
        "        # in the forward pass. The memory parameters are updated via backprop.\n",
        "        read_memory_output = self.memory_unit(candidate_item_emb, historical_item_embs)\n",
        "\n",
        "        # Concatenate all features\n",
        "        concatenated_features = torch.cat([\n",
        "            user_emb,\n",
        "            candidate_item_emb,\n",
        "            read_memory_output\n",
        "        ], dim=-1)\n",
        "\n",
        "        # Pass through prediction MLP\n",
        "        mlp_output = self.mlp(concatenated_features)\n",
        "        logits = self.output_layer(mlp_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Instantiate and move model to device\n",
        "mimn_model = MIMN(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    max_history_length=max_history_length,\n",
        "    embedding_dim=embedding_dim,\n",
        "    memory_slots=memory_slots,\n",
        "    memory_dim=memory_dim,\n",
        "    item_features_matrix=item_features_data_np\n",
        ").to(device)\n",
        "\n",
        "print(\"\\nMIMN Model Summary:\")\n",
        "print(mimn_model)\n",
        "\n",
        "\n",
        "# --- 3. Prepare Data for Training (PyTorch DataLoader) ---\n",
        "dataset = TensorDataset(user_ids_data, candidate_item_ids_data, historical_item_ids_data, labels_data)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- 4. Train the Model ---\n",
        "optimizer = torch.optim.Adam(mimn_model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "print(f\"\\n--- Training the MIMN Model for {num_epochs} epochs ---\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    mimn_model.train()\n",
        "    total_loss = 0\n",
        "    predictions_train = []\n",
        "    labels_train = []\n",
        "\n",
        "    for batch_idx, (user_ids, candidate_ids, historical_ids, labels) in enumerate(train_loader):\n",
        "        user_ids, candidate_ids, historical_ids, labels = \\\n",
        "            user_ids.to(device), candidate_ids.to(device), historical_ids.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mimn_model(user_ids, candidate_ids, historical_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        predictions_train.extend(outputs.sigmoid().detach().cpu().numpy().flatten())\n",
        "        labels_train.extend(labels.detach().cpu().numpy().flatten())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_accuracy = accuracy_score(labels_train, np.round(predictions_train))\n",
        "    train_auc = roc_auc_score(labels_train, predictions_train)\n",
        "\n",
        "    # --- Validation ---\n",
        "    mimn_model.eval()\n",
        "    val_total_loss = 0\n",
        "    predictions_val = []\n",
        "    labels_val = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user_ids, candidate_ids, historical_ids, labels in val_loader:\n",
        "            user_ids, candidate_ids, historical_ids, labels = \\\n",
        "                user_ids.to(device), candidate_ids.to(device), historical_ids.to(device), labels.to(device)\n",
        "\n",
        "            outputs = mimn_model(user_ids, candidate_ids, historical_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_total_loss += loss.item()\n",
        "\n",
        "            predictions_val.extend(outputs.sigmoid().cpu().numpy().flatten())\n",
        "            labels_val.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "    avg_val_loss = val_total_loss / len(val_loader)\n",
        "    val_accuracy = accuracy_score(labels_val, np.round(predictions_val))\n",
        "    val_auc = roc_auc_score(labels_val, predictions_val)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train AUC: {train_auc:.4f}\")\n",
        "    print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "\n",
        "# --- 5. Make Predictions (Example) ---\n",
        "print(\"\\n--- Making Predictions (Example) ---\")\n",
        "\n",
        "mimn_model.eval()\n",
        "\n",
        "num_predict_samples = 5\n",
        "random_indices = np.random.choice(len(user_ids_data), num_predict_samples, replace=False)\n",
        "\n",
        "sample_user_ids = user_ids_data[random_indices].to(device)\n",
        "sample_candidate_item_ids = candidate_item_ids_data[random_indices].to(device)\n",
        "sample_historical_item_ids = historical_item_ids_data[random_indices].to(device)\n",
        "sample_labels = labels_data[random_indices].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_outputs = mimn_model(sample_user_ids, sample_candidate_item_ids, sample_historical_item_ids)\n",
        "    sample_predictions = sample_outputs.sigmoid().cpu().numpy().flatten()\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(num_predict_samples):\n",
        "    print(f\"  Sample {i+1}:\")\n",
        "    print(f\"    User ID: {sample_user_ids[i].item()}\")\n",
        "    print(f\"    Candidate Item ID: {sample_candidate_item_ids[i].item()}\")\n",
        "    print(f\"    Historical Item IDs: {sample_historical_item_ids[i].cpu().numpy().tolist()}\")\n",
        "    print(f\"    True Label: {sample_labels[i].item()}\")\n",
        "    print(f\"    Predicted Probability: {sample_predictions[i]:.4f}\")\n",
        "    print(\"-\" * 30)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124\n",
            "Using device: cpu\n",
            "Generated synthetic data: 50000 samples.\n",
            "  User IDs shape: (50000,)\n",
            "  Candidate Item IDs shape: (50000,)\n",
            "  Historical Item IDs shape: (50000, 50)\n",
            "  Labels shape: (50000,)\n",
            "  Item Features matrix shape: (501, 16)\n",
            "\n",
            "MIMN Model Summary:\n",
            "MIMN(\n",
            "  (user_embedding): Embedding(1001, 16)\n",
            "  (item_embedding): Embedding(501, 16)\n",
            "  (memory_unit): MemoryUnit(\n",
            "    (attention_mlp): Sequential(\n",
            "      (0): Linear(in_features=48, out_features=64, bias=True)\n",
            "      (1): Dice()\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "    (memory_update_gru): GRUCell(48, 32)\n",
            "  )\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (7): ReLU()\n",
            "  )\n",
            "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "--- Training the MIMN Model for 5 epochs ---\n",
            "Epoch 1/5:\n",
            "  Train Loss: 0.6933, Train Acc: 0.4996, Train AUC: 0.4982\n",
            "  Val Loss: 0.6933, Val Acc: 0.4978, Val AUC: 0.4966\n",
            "Epoch 2/5:\n",
            "  Train Loss: 0.6932, Train Acc: 0.5050, Train AUC: 0.5056\n",
            "  Val Loss: 0.6932, Val Acc: 0.4987, Val AUC: 0.4959\n",
            "Epoch 3/5:\n",
            "  Train Loss: 0.6929, Train Acc: 0.5073, Train AUC: 0.5110\n",
            "  Val Loss: 0.6932, Val Acc: 0.5039, Val AUC: 0.5033\n",
            "Epoch 4/5:\n",
            "  Train Loss: 0.6928, Train Acc: 0.5150, Train AUC: 0.5177\n",
            "  Val Loss: 0.6933, Val Acc: 0.5057, Val AUC: 0.4989\n",
            "Epoch 5/5:\n",
            "  Train Loss: 0.6923, Train Acc: 0.5159, Train AUC: 0.5229\n",
            "  Val Loss: 0.6939, Val Acc: 0.4946, Val AUC: 0.4961\n",
            "\n",
            "Training complete.\n",
            "\n",
            "--- Making Predictions (Example) ---\n",
            "\n",
            "Sample Predictions:\n",
            "  Sample 1:\n",
            "    User ID: 882\n",
            "    Candidate Item ID: 471\n",
            "    Historical Item IDs: [444, 125, 123, 44, 236, 491, 210, 272, 492, 230, 132, 165, 212, 338, 295, 364, 392, 3, 412, 221, 106, 434, 463, 202, 52, 288, 368, 26, 377, 7, 103, 400, 162, 219, 361, 409, 90, 323, 297, 40, 450, 417, 25, 0, 0, 0, 0, 0, 0, 0]\n",
            "    True Label: 0.0\n",
            "    Predicted Probability: 0.5213\n",
            "------------------------------\n",
            "  Sample 2:\n",
            "    User ID: 21\n",
            "    Candidate Item ID: 123\n",
            "    Historical Item IDs: [252, 285, 244, 277, 181, 254, 230, 268, 255, 26, 184, 181, 61, 336, 5, 347, 172, 439, 438, 488, 161, 206, 417, 79, 103, 235, 230, 24, 161, 83, 88, 82, 43, 353, 12, 166, 302, 147, 324, 193, 348, 356, 132, 183, 188, 402, 0, 0, 0, 0]\n",
            "    True Label: 0.0\n",
            "    Predicted Probability: 0.5201\n",
            "------------------------------\n",
            "  Sample 3:\n",
            "    User ID: 178\n",
            "    Candidate Item ID: 184\n",
            "    Historical Item IDs: [141, 219, 115, 414, 138, 375, 160, 416, 96, 432, 286, 414, 358, 179, 293, 263, 121, 277, 430, 358, 479, 192, 424, 92, 383, 392, 414, 346, 473, 269, 248, 78, 54, 332, 363, 400, 152, 112, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    True Label: 0.0\n",
            "    Predicted Probability: 0.5168\n",
            "------------------------------\n",
            "  Sample 4:\n",
            "    User ID: 98\n",
            "    Candidate Item ID: 314\n",
            "    Historical Item IDs: [384, 2, 400, 82, 269, 428, 119, 204, 348, 182, 154, 35, 113, 9, 351, 334, 42, 400, 362, 96, 472, 353, 447, 489, 27, 150, 52, 111, 344, 158, 43, 206, 468, 142, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    True Label: 1.0\n",
            "    Predicted Probability: 0.5114\n",
            "------------------------------\n",
            "  Sample 5:\n",
            "    User ID: 382\n",
            "    Candidate Item ID: 262\n",
            "    Historical Item IDs: [404, 45, 374, 47, 164, 245, 320, 189, 227, 251, 263, 468, 447, 225, 169, 142, 338, 254, 218, 268, 99, 331, 23, 56, 25, 42, 96, 367, 372, 442, 164, 110, 465, 232, 383, 125, 165, 352, 299, 448, 290, 98, 328, 147, 409, 377, 309, 498, 261, 417]\n",
            "    True Label: 0.0\n",
            "    Predicted Probability: 0.4919\n",
            "------------------------------\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZICRZYpwr61",
        "outputId": "cc8b50f6-a7ab-4ca1-ec08-4c415612f339"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RtewZ4g5OTZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}