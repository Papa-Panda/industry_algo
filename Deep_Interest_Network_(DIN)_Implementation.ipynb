{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/industry_algo/blob/main/Deep_Interest_Network_(DIN)_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gemini.google.com/app/72f602f1dcb0baa4\n",
        "\n",
        "# 屠龙少年与龙：漫谈深度学习驱动的广告推荐技术发展周期 - 朱小强的文章 - 知乎\n",
        "# https://zhuanlan.zhihu.com/p/398041971\n",
        "# 推荐系统中的注意力机制——阿里深度兴趣网络（DIN） - 王喆的文章 - 知乎\n",
        "# https://zhuanlan.zhihu.com/p/51623339"
      ],
      "metadata": {
        "id": "NVQSI-ivK0Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Synthetic Data Generation ---\n",
        "# This dataset simulates user interactions with items, adapted for PyTorch.\n",
        "\n",
        "def generate_synthetic_data(num_samples=10000, num_users=1000, num_items=500, history_length=10, embedding_dim=16):\n",
        "    \"\"\"\n",
        "    Generates synthetic data for a Deep Interest Network.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): Total number of data points (user-candidate pairs).\n",
        "        num_users (int): Number of unique users.\n",
        "        num_items (int): Number of unique items.\n",
        "        history_length (int): Maximum length of a user's historical clicked items.\n",
        "        embedding_dim (int): Dimensionality of item and user embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing PyTorch Tensors:\n",
        "            - user_ids (torch.Tensor): User IDs.\n",
        "            - candidate_item_ids (torch.Tensor): Candidate Item IDs.\n",
        "            - historical_item_ids (torch.Tensor): 2D array of Historical Item IDs (padded with 0s).\n",
        "            - labels (torch.Tensor): Labels (1 if clicked, 0 otherwise).\n",
        "            - item_features_matrix (np.array): Item feature embeddings (simulated, numpy for embedding init).\n",
        "    \"\"\"\n",
        "    user_ids = np.random.randint(1, num_users + 1, num_samples) # User IDs start from 1\n",
        "    candidate_item_ids = np.random.randint(1, num_items + 1, num_samples) # Item IDs start from 1\n",
        "\n",
        "    historical_item_ids = []\n",
        "    for _ in range(num_samples):\n",
        "        current_history_len = np.random.randint(1, history_length + 1)\n",
        "        history = np.random.randint(1, num_items + 1, current_history_len).tolist()\n",
        "        history.extend([0] * (history_length - len(history))) # Pad with 0s\n",
        "        historical_item_ids.append(history)\n",
        "    historical_item_ids = np.array(historical_item_ids)\n",
        "\n",
        "    labels = np.random.randint(0, 2, num_samples)\n",
        "\n",
        "    item_features_matrix = np.random.rand(num_items + 1, embedding_dim).astype(np.float32) # +1 for 0-padding, item 0 is dummy\n",
        "\n",
        "    print(f\"Generated synthetic data: {num_samples} samples.\")\n",
        "    print(f\"  User IDs shape: {user_ids.shape}\")\n",
        "    print(f\"  Candidate Item IDs shape: {candidate_item_ids.shape}\")\n",
        "    print(f\"  Historical Item IDs shape: {historical_item_ids.shape}\")\n",
        "    print(f\"  Labels shape: {labels.shape}\")\n",
        "    print(f\"  Item Features matrix shape: {item_features_matrix.shape}\")\n",
        "\n",
        "    # Convert to PyTorch Tensors\n",
        "    user_ids_t = torch.LongTensor(user_ids)\n",
        "    candidate_item_ids_t = torch.LongTensor(candidate_item_ids)\n",
        "    historical_item_ids_t = torch.LongTensor(historical_item_ids)\n",
        "    labels_t = torch.FloatTensor(labels).unsqueeze(1) # Add a dimension for BCEWithLogitsLoss\n",
        "\n",
        "    return user_ids_t, candidate_item_ids_t, historical_item_ids_t, labels_t, item_features_matrix\n",
        "\n",
        "# Generate the data\n",
        "num_users = 1000\n",
        "num_items = 500\n",
        "history_length = 10 # Max length of user behavior sequence\n",
        "embedding_dim = 16 # Dimensionality of item and user embeddings\n",
        "\n",
        "user_ids_data, candidate_item_ids_data, historical_item_ids_data, labels_data, item_features_data_np = \\\n",
        "    generate_synthetic_data(num_samples=50000, num_users=num_users, num_items=num_items, history_length=history_length, embedding_dim=embedding_dim)\n",
        "\n",
        "# --- 2. DIN Model Architecture in PyTorch ---\n",
        "\n",
        "# Custom Activation Function (Dice)\n",
        "class Dice(nn.Module):\n",
        "    \"\"\"\n",
        "    Data Adaptive Activation Function (DICE) for DIN in PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, epsilon=1e-9):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        # alphas and beta are learnable parameters, one for each feature dimension\n",
        "        self.alphas = nn.Parameter(torch.zeros(input_dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(input_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate mean and variance along batch and sequence dimensions,\n",
        "        # keeping the feature dimension intact.\n",
        "        # Example: if x is (batch_size, seq_len, features_dim),\n",
        "        # mean and variance will be (1, 1, features_dim)\n",
        "        reduction_axes = tuple(range(x.dim() - 1))\n",
        "        mean = torch.mean(x, dim=reduction_axes, keepdim=True)\n",
        "        variance = torch.mean(torch.pow(x - mean, 2), dim=reduction_axes, keepdim=True)\n",
        "\n",
        "        x_normed = (x - mean) / torch.sqrt(variance + self.epsilon)\n",
        "\n",
        "        # p is calculated element-wise across the feature dimension\n",
        "        p = torch.sigmoid(self.alphas * x_normed + self.beta)\n",
        "\n",
        "        # Apply DICE activation formula\n",
        "        return p * x + (1 - p) * self.alphas * x\n",
        "\n",
        "# Attention Pooling Layer for DIN\n",
        "class AttentionPoolingLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention pooling layer for Deep Interest Network (DIN) in PyTorch.\n",
        "    Calculates attention scores between candidate item and historical items,\n",
        "    then weights the historical item embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, hidden_units=[80, 40]):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "        # Attention network (MLP)\n",
        "        attention_mlp_layers = []\n",
        "        input_dim_mlp = embedding_dim * 4 # Concatenated features: candidate, history, product, difference\n",
        "\n",
        "        for i, units in enumerate(hidden_units):\n",
        "            attention_mlp_layers.append(nn.Linear(input_dim_mlp, units))\n",
        "            attention_mlp_layers.append(Dice(units)) # Apply Dice activation\n",
        "            input_dim_mlp = units # Update input_dim for the next layer\n",
        "\n",
        "        self.attention_mlp = nn.Sequential(*attention_mlp_layers)\n",
        "        self.output_layer = nn.Linear(input_dim_mlp, 1) # Outputs a single attention score\n",
        "\n",
        "    def forward(self, candidate_item_embedding, historical_item_embeddings):\n",
        "        # candidate_item_embedding: (batch_size, embedding_dim)\n",
        "        # historical_item_embeddings: (batch_size, history_length, embedding_dim)\n",
        "\n",
        "        batch_size, history_length, _ = historical_item_embeddings.shape\n",
        "\n",
        "        # Expand candidate_item_embedding to match history_length dimension for concatenation\n",
        "        # (batch_size, 1, embedding_dim) -> (batch_size, history_length, embedding_dim)\n",
        "        candidate_item_embedding_tiled = candidate_item_embedding.unsqueeze(1).expand(-1, history_length, -1)\n",
        "\n",
        "        # Concatenate candidate item, historical item, their product, and their difference\n",
        "        # Resulting shape: (batch_size, history_length, embedding_dim * 4)\n",
        "        concatenated_features = torch.cat([\n",
        "            candidate_item_embedding_tiled,\n",
        "            historical_item_embeddings,\n",
        "            candidate_item_embedding_tiled * historical_item_embeddings,\n",
        "            candidate_item_embedding_tiled - historical_item_embeddings\n",
        "        ], dim=-1)\n",
        "\n",
        "        # Pass through attention network\n",
        "        attention_logits = self.attention_mlp(concatenated_features) # (batch_size, history_length, hidden_units[-1])\n",
        "        attention_logits = self.output_layer(attention_logits) # (batch_size, history_length, 1)\n",
        "\n",
        "        # Mask out padded items (where the historical_item_embeddings are all zeros)\n",
        "        # Create a mask: sum of absolute values along embedding dim will be zero for padded items\n",
        "        mask = (historical_item_embeddings.abs().sum(dim=-1, keepdim=True) > 0).float()\n",
        "        attention_logits = attention_logits - (1.0 - mask) * 1e9 # Apply large negative value to masked logits\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(attention_logits, dim=1) # (batch_size, history_length, 1)\n",
        "\n",
        "        # Weighted sum of historical item embeddings\n",
        "        # (batch_size, history_length, embedding_dim) * (batch_size, history_length, 1)\n",
        "        # -> (batch_size, history_length, embedding_dim) -> sum over history_length -> (batch_size, embedding_dim)\n",
        "        weighted_history_embedding = torch.sum(attention_weights * historical_item_embeddings, dim=1)\n",
        "\n",
        "        return weighted_history_embedding\n",
        "\n",
        "# Deep Interest Network (DIN) Model\n",
        "class DIN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, history_length, embedding_dim, item_features_matrix):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.history_length = history_length\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Embedding layers\n",
        "        self.user_embedding = nn.Embedding(num_users + 1, embedding_dim)\n",
        "        # Initialize item embedding with pre-defined matrix (e.g., from pre-training)\n",
        "        # Ensure it's a float tensor for embedding weights\n",
        "        self.item_embedding = nn.Embedding.from_pretrained(\n",
        "            torch.from_numpy(item_features_matrix).float(),\n",
        "            freeze=False # Allow fine-tuning during training\n",
        "        )\n",
        "\n",
        "        # DIN Attention Pooling Layer\n",
        "        self.attention_pooling_layer = AttentionPoolingLayer(embedding_dim=embedding_dim)\n",
        "\n",
        "        # Final Prediction MLP\n",
        "        # Input dim for MLP: user_embedding (embedding_dim) + candidate_item_embedding (embedding_dim)\n",
        "        #                     + attention_output (embedding_dim)\n",
        "        input_mlp_dim = embedding_dim * 3\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_mlp_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # Output layer with sigmoid for binary classification probability\n",
        "        self.output_layer = nn.Linear(32, 1)\n",
        "        # Note: We will use BCEWithLogitsLoss which combines sigmoid and BCELoss for numerical stability,\n",
        "        # so the final activation here will *not* be sigmoid, it's implicitly handled by the loss.\n",
        "\n",
        "    def forward(self, user_id, candidate_item_id, historical_item_ids):\n",
        "        # Ensure inputs are 1D for embedding lookup\n",
        "        user_id = user_id.squeeze(-1) # (batch_size, 1) -> (batch_size,)\n",
        "        candidate_item_id = candidate_item_id.squeeze(-1) # (batch_size, 1) -> (batch_size,)\n",
        "\n",
        "        # Embeddings lookup\n",
        "        user_emb = self.user_embedding(user_id) # (batch_size, embedding_dim)\n",
        "        candidate_item_emb = self.item_embedding(candidate_item_id) # (batch_size, embedding_dim)\n",
        "        historical_item_embs = self.item_embedding(historical_item_ids) # (batch_size, history_length, embedding_dim)\n",
        "\n",
        "        # Pass through Attention Pooling Layer\n",
        "        attention_output = self.attention_pooling_layer(candidate_item_emb, historical_item_embs)\n",
        "\n",
        "        # Concatenate all features\n",
        "        concatenated_features = torch.cat([\n",
        "            user_emb,\n",
        "            candidate_item_emb,\n",
        "            attention_output\n",
        "        ], dim=-1)\n",
        "\n",
        "        # Pass through prediction MLP\n",
        "        mlp_output = self.mlp(concatenated_features)\n",
        "        # Final output (logits)\n",
        "        logits = self.output_layer(mlp_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Instantiate and move model to device\n",
        "din_model = DIN(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    history_length=history_length,\n",
        "    embedding_dim=embedding_dim,\n",
        "    item_features_matrix=item_features_data_np\n",
        ").to(device)\n",
        "\n",
        "print(\"\\nDIN Model Summary:\")\n",
        "# A simple way to print model structure, similar to Keras summary\n",
        "print(din_model)\n",
        "# You might need to pass a dummy input to get details for each layer.\n",
        "# print(din_model(torch.zeros(2,1).long().to(device), torch.zeros(2,1).long().to(device), torch.zeros(2, history_length).long().to(device)))\n",
        "\n",
        "\n",
        "# --- 3. Prepare Data for Training (PyTorch DataLoader) ---\n",
        "dataset = TensorDataset(user_ids_data, candidate_item_ids_data, historical_item_ids_data, labels_data)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- 4. Train the Model ---\n",
        "optimizer = torch.optim.Adam(din_model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss() # Combines Sigmoid and Binary Cross-Entropy for numerical stability\n",
        "\n",
        "num_epochs = 5\n",
        "print(f\"\\n--- Training the DIN Model for {num_epochs} epochs ---\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    din_model.train() # Set model to training mode\n",
        "    total_loss = 0\n",
        "    predictions_train = []\n",
        "    labels_train = []\n",
        "\n",
        "    for batch_idx, (user_ids, candidate_ids, historical_ids, labels) in enumerate(train_loader):\n",
        "        user_ids, candidate_ids, historical_ids, labels = \\\n",
        "            user_ids.to(device), candidate_ids.to(device), historical_ids.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "        outputs = din_model(user_ids, candidate_ids, historical_ids) # Forward pass\n",
        "        loss = criterion(outputs, labels) # Calculate loss\n",
        "        loss.backward() # Backward pass\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Store predictions and labels for metrics\n",
        "        predictions_train.extend(outputs.sigmoid().detach().cpu().numpy().flatten())\n",
        "        labels_train.extend(labels.detach().cpu().numpy().flatten())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_accuracy = accuracy_score(labels_train, np.round(predictions_train))\n",
        "    train_auc = roc_auc_score(labels_train, predictions_train)\n",
        "\n",
        "    # --- Validation ---\n",
        "    din_model.eval() # Set model to evaluation mode\n",
        "    val_total_loss = 0\n",
        "    predictions_val = []\n",
        "    labels_val = []\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for user_ids, candidate_ids, historical_ids, labels in val_loader:\n",
        "            user_ids, candidate_ids, historical_ids, labels = \\\n",
        "                user_ids.to(device), candidate_ids.to(device), historical_ids.to(device), labels.to(device)\n",
        "\n",
        "            outputs = din_model(user_ids, candidate_ids, historical_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_total_loss += loss.item()\n",
        "\n",
        "            predictions_val.extend(outputs.sigmoid().cpu().numpy().flatten())\n",
        "            labels_val.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "    avg_val_loss = val_total_loss / len(val_loader)\n",
        "    val_accuracy = accuracy_score(labels_val, np.round(predictions_val))\n",
        "    val_auc = roc_auc_score(labels_val, predictions_val)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train AUC: {train_auc:.4f}\")\n",
        "    print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "\n",
        "# --- 5. Make Predictions (Example) ---\n",
        "print(\"\\n--- Making Predictions (Example) ---\")\n",
        "\n",
        "din_model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Select a few random samples for prediction\n",
        "num_predict_samples = 5\n",
        "random_indices = np.random.choice(len(user_ids_data), num_predict_samples, replace=False)\n",
        "\n",
        "sample_user_ids = user_ids_data[random_indices].to(device)\n",
        "sample_candidate_item_ids = candidate_item_ids_data[random_indices].to(device)\n",
        "sample_historical_item_ids = historical_item_ids_data[random_indices].to(device)\n",
        "sample_labels = labels_data[random_indices].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_outputs = din_model(sample_user_ids, sample_candidate_item_ids, sample_historical_item_ids)\n",
        "    sample_predictions = sample_outputs.sigmoid().cpu().numpy().flatten() # Apply sigmoid to get probabilities\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(num_predict_samples):\n",
        "    print(f\"  Sample {i+1}:\")\n",
        "    print(f\"    User ID: {sample_user_ids[i].item()}\")\n",
        "    print(f\"    Candidate Item ID: {sample_candidate_item_ids[i].item()}\")\n",
        "    print(f\"    Historical Item IDs: {sample_historical_item_ids[i].cpu().numpy().tolist()}\")\n",
        "    print(f\"    True Label: {sample_labels[i].item()}\")\n",
        "    print(f\"    Predicted Probability: {sample_predictions[i]:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n"
      ],
      "metadata": {
        "id": "DXwHrD7bK1mA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a6d447-4b34-432e-e50f-36a921dbaf72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124\n",
            "Using device: cpu\n",
            "Generated synthetic data: 50000 samples.\n",
            "  User IDs shape: (50000,)\n",
            "  Candidate Item IDs shape: (50000,)\n",
            "  Historical Item IDs shape: (50000, 10)\n",
            "  Labels shape: (50000,)\n",
            "  Item Features matrix shape: (501, 16)\n",
            "\n",
            "DIN Model Summary:\n",
            "DIN(\n",
            "  (user_embedding): Embedding(1001, 16)\n",
            "  (item_embedding): Embedding(501, 16)\n",
            "  (attention_pooling_layer): AttentionPoolingLayer(\n",
            "    (attention_mlp): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
            "      (1): Dice()\n",
            "      (2): Linear(in_features=80, out_features=40, bias=True)\n",
            "      (3): Dice()\n",
            "    )\n",
            "    (output_layer): Linear(in_features=40, out_features=1, bias=True)\n",
            "  )\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=48, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (7): ReLU()\n",
            "  )\n",
            "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "--- Training the DIN Model for 5 epochs ---\n",
            "Epoch 1/5:\n",
            "  Train Loss: 0.6935, Train Acc: 0.5018, Train AUC: 0.4985\n",
            "  Val Loss: 0.6934, Val Acc: 0.4926, Val AUC: 0.4912\n",
            "Epoch 2/5:\n",
            "  Train Loss: 0.6932, Train Acc: 0.5071, Train AUC: 0.5057\n",
            "  Val Loss: 0.6934, Val Acc: 0.4929, Val AUC: 0.4944\n",
            "Epoch 3/5:\n",
            "  Train Loss: 0.6929, Train Acc: 0.5113, Train AUC: 0.5127\n",
            "  Val Loss: 0.6940, Val Acc: 0.4949, Val AUC: 0.4901\n",
            "Epoch 4/5:\n",
            "  Train Loss: 0.6926, Train Acc: 0.5171, Train AUC: 0.5211\n",
            "  Val Loss: 0.6940, Val Acc: 0.4917, Val AUC: 0.4885\n",
            "Epoch 5/5:\n",
            "  Train Loss: 0.6917, Train Acc: 0.5216, Train AUC: 0.5302\n",
            "  Val Loss: 0.6946, Val Acc: 0.4885, Val AUC: 0.4884\n",
            "\n",
            "Training complete.\n",
            "\n",
            "--- Making Predictions (Example) ---\n",
            "\n",
            "Sample Predictions:\n",
            "  Sample 1:\n",
            "    User ID: 568\n",
            "    Candidate Item ID: 198\n",
            "    Historical Item IDs: [43, 96, 363, 297, 2, 0, 0, 0, 0, 0]\n",
            "    True Label: 0.0\n",
            "    Predicted Probability: 0.5201\n",
            "------------------------------\n",
            "  Sample 2:\n",
            "    User ID: 348\n",
            "    Candidate Item ID: 205\n",
            "    Historical Item IDs: [402, 196, 272, 2, 124, 348, 257, 47, 288, 42]\n",
            "    True Label: 1.0\n",
            "    Predicted Probability: 0.5062\n",
            "------------------------------\n",
            "  Sample 3:\n",
            "    User ID: 110\n",
            "    Candidate Item ID: 449\n",
            "    Historical Item IDs: [273, 157, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    True Label: 1.0\n",
            "    Predicted Probability: 0.4823\n",
            "------------------------------\n",
            "  Sample 4:\n",
            "    User ID: 384\n",
            "    Candidate Item ID: 118\n",
            "    Historical Item IDs: [171, 488, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    True Label: 0.0\n",
            "    Predicted Probability: 0.4908\n",
            "------------------------------\n",
            "  Sample 5:\n",
            "    User ID: 73\n",
            "    Candidate Item ID: 490\n",
            "    Historical Item IDs: [316, 55, 186, 121, 226, 264, 13, 256, 400, 0]\n",
            "    True Label: 1.0\n",
            "    Predicted Probability: 0.5139\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZV-Cu4uw--B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}